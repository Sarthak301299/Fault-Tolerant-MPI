
@article{hargrove_berkeley_2006,
	title = {Berkeley lab checkpoint/restart ({BLCR}) for {Linux} clusters},
	volume = {46},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/46/1/067},
	doi = {10.1088/1742-6596/46/1/067},
	urldate = {2022-09-04},
	journal = {Journal of Physics: Conference Series},
	author = {Hargrove, Paul H and Duell, Jason C},
	month = sep,
	year = {2006},
	pages = {494--499},
}

@article{sankaran_lam/mpi_2005,
	title = {The {Lam}/{Mpi} {Checkpoint}/{Restart} {Framework}: {System}-{Initiated} {Checkpointing}},
	volume = {19},
	issn = {1094-3420, 1741-2846},
	shorttitle = {The {Lam}/{Mpi} {Checkpoint}/{Restart} {Framework}},
	url = {http://journals.sagepub.com/doi/10.1177/1094342005056139},
	doi = {10.1177/1094342005056139},
	abstract = {As high performance clusters continue to grow in size and popularity, issues of fault tolerance and reliability are becoming limiting factors on application scalability. To address these issues, we present the design and implementation of a system for providing coordinated checkpointing and rollback recovery for MPI-based parallel applications. Our approach integrates the Berkeley Lab BLCR kernel-level process checkpoint system with the LAM implementation of MPI through a defined checkpoint/restart interface. Checkpointing is transparent to the application, allowing the system to be used for cluster maintenance and scheduling reasons as well as for fault tolerance. Experimental results show negligible communication performance impact due to the incorporation of the checkpoint support capabilities into LAM/MPI.},
	language = {en},
	number = {4},
	urldate = {2022-09-04},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Sankaran, Sriram and Squyres, Jeffrey M. and Barrett, Brian and Sahay, Vishal and Lumsdaine, Andrew and Duell, Jason and Hargrove, Paul and Roman, Eric},
	month = nov,
	year = {2005},
	pages = {479--493},
}

@article{cappello_fault_2009,
	title = {Fault {Tolerance} in {Petascale}/ {Exascale} {Systems}: {Current} {Knowledge}, {Challenges} and {Research} {Opportunities}},
	volume = {23},
	issn = {1094-3420, 1741-2846},
	shorttitle = {Fault {Tolerance} in {Petascale}/ {Exascale} {Systems}},
	url = {http://journals.sagepub.com/doi/10.1177/1094342009106189},
	doi = {10.1177/1094342009106189},
	abstract = {The emergence of petascale systems and the promise of future exascale systems have reinvigorated the community interest in how to manage failures in such systems and ensure that large applications, lasting several hours or tens of hours, are completed successfully. Most of the existing results for several key mechanisms associated with fault tolerance in high-performance computing (HPC) platforms follow the rollback—recovery approach. Over the last decade, these mechanisms have received a lot of attention from the community with different levels of success. Unfortunately, despite their high degree of optimization, existing approaches do not fit well with the challenging evolutions of large-scale systems. There is room and even a need for new approaches. Opportunities may come from different origins: diskless checkpointing, algorithmic-based fault tolerance, proactive operation, speculative execution, software transactional memory, forward recovery, etc. The contributions of this paper are as follows: (1) we summarize and analyze the existing results concerning the failures in large-scale computers and point out the urgent need for drastic improvements or disruptive approaches for fault tolerance in these systems; (2) we sketch most of the known opportunities and analyze their associated limitations; (3) we extract and express the challenges that the HPC community will have to face for addressing the stringent issue of failures in HPC systems.},
	language = {en},
	number = {3},
	urldate = {2022-09-04},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Cappello, Franck},
	month = aug,
	year = {2009},
	pages = {212--226},
}

@inproceedings{benoit_replication_2019,
	address = {New York, NY, USA},
	series = {{SC} '19},
	title = {Replication is more efficient than you think},
	isbn = {9781450362290},
	url = {https://doi.org/10.1145/3295500.3356171},
	doi = {10.1145/3295500.3356171},
	abstract = {This paper revisits replication coupled with checkpointing for fail-stop errors. Replication enables the application to survive many fail-stop errors, thereby allowing for longer checkpointing periods. Previously published works use replication with the no-restart strategy, which works as follows: (i) compute the application Mean Time To Interruption (MTTI) M as a function of the number of processor pairs and the individual processor Mean Time Between Failures (MTBF); (ii) use checkpointing period [EQUATION] à la Young/Daly, where C is the checkpoint duration; and (iii) never restart failed processors until the application crashes. We introduce the restart strategy where failed processors are restarted after each checkpoint. We compute the optimal checkpointing period [EQUATION] for this strategy, which is much larger than [EQUATION], thereby decreasing I/O pressure. We show through simulations that using [EQUATION] and the restart strategy, instead of [EQUATION] and the usual no-restart strategy, significantly decreases the overhead induced by replication.},
	urldate = {2022-09-04},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Benoit, Anne and Herault, Thomas and Fèvre, Valentin Le and Robert, Yves},
	month = nov,
	year = {2019},
	pages = {1--14},
}

@article{walters_replication-based_2009,
	title = {Replication-{Based} {Fault} {Tolerance} for {MPI} {Applications}},
	volume = {20},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2008.172},
	abstract = {As computational clusters increase in size, their mean time to failure reduces drastically. Typically, checkpointing is used to minimize the loss of computation. Most checkpointing techniques, however, require central storage for storing checkpoints. This results in a bottleneck and severely limits the scalability of checkpointing, while also proving to be too expensive for dedicated checkpointing networks and storage systems. We propose a scalable replication-based MPI checkpointing facility. Our reference implementation is based on LAM/MPI; however, it is directly applicable to any MPI implementation. We extend the existing state of fault-tolerant MPI with asynchronous replication, eliminating the need for central or network storage. We evaluate centralized storage, a Sun-X4500-based solution, an EMC storage area network (SAN), and the Ibrix commercial parallel file system and show that they are not scalable, particularly after 64 CPUs. We demonstrate the low overhead of our checkpointing and replication scheme with the NAS Parallel Benchmarks and the High-Performance LINPACK benchmark with tests up to 256 nodes while demonstrating that checkpointing and replication can be achieved with a much lower overhead than that provided by current techniques. Finally, we show that the monetary cost of our solution is as low as 25 percent of that of a typical SAN/parallel-file-system-equipped storage system.},
	number = {7},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Walters, John Paul and Chaudhary, Vipin},
	month = jul,
	year = {2009},
	keywords = {Fault tolerance, Checkpointing, File systems, Storage area networks, Benchmark testing, Network servers, File servers, Scalability, Computer networks, Concurrent computing, Fault tolerance, checkpointing, MPI, file systems., fault-tolerance, checkpointing, MPI, file systems},
	pages = {997--1010},
}

@article{george_fault_2015,
	title = {Fault {Tolerance} on {Large} {Scale} {Systems} using {Adaptive} {Process} {Replication}},
	volume = {64},
	issn = {1557-9956},
	doi = {10.1109/TC.2014.2360536},
	abstract = {Exascale systems of the future are predicted to have mean time between failures (MTBF) of less than one hour. At such low MTBFs, employing periodic checkpointing alone will result in low efficiency because of the high number of application failures resulting in large amount of lost work due to rollbacks. In such scenarios, it is highly necessary to have proactive fault tolerance mechanisms that can help avoid significant number of failures. In this work, we have developed a mechanism for proactive fault tolerance using partial replication of a set of application processes. Our fault tolerance framework adaptively changes the set of replicated processes periodically based on failure predictions to avoid failures. We have developed an MPI prototype implementation, PAREP-MPI that allows changing the replica set. We have shown that our strategy involving adaptive process replication significantly outperforms existing mechanisms providing up to 20 percent improvement in application efficiency even for exascale systems.},
	number = {8},
	journal = {IEEE Transactions on Computers},
	author = {George, Cijo and Vadhiyar, Sathish},
	month = aug,
	year = {2015},
	keywords = {Checkpointing, Fault tolerant systems, Redundancy, Receivers, Large-scale systems, Libraries, Fault tolerance, Process replication, Exascale systems, Fault tolerance, process replication, exascale systems},
	pages = {2213--2225},
}

@article{bland_post-failure_2013,
	title = {Post-failure recovery of {MPI} communication capability: {Design} and rationale},
	volume = {27},
	issn = {1094-3420, 1741-2846},
	shorttitle = {Post-failure recovery of {MPI} communication capability},
	url = {http://journals.sagepub.com/doi/10.1177/1094342013488238},
	doi = {10.1177/1094342013488238},
	abstract = {As supercomputers are entering an era of massive parallelism where the frequency of faults is increasing, the MPI Standard remains distressingly vague on the consequence of failures on MPI communications. Advanced fault-tolerance techniques have the potential to prevent full-scale application restart and therefore lower the cost incurred for each failure, but they demand from MPI the capability to detect failures and resume communications afterward. In this paper, we present a set of extensions to MPI that allow communication capabilities to be restored, while maintaining the extreme level of performance to which MPI users have become accustomed. The motivation behind the design choices are weighted against alternatives, a task that requires simultaneously considering MPI from the viewpoint of both the user and the implementor. The usability of the interfaces for expressing advanced recovery techniques is then discussed, including the difficult issue of enabling separate software layers to coordinate their recovery.},
	language = {en},
	number = {3},
	urldate = {2022-09-04},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Bland, Wesley and Bouteiller, Aurelien and Herault, Thomas and Bosilca, George and Dongarra, Jack},
	month = aug,
	year = {2013},
	pages = {244--254},
}

@inproceedings{gabriel_open_2004,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Open {MPI}: {Goals}, {Concept}, and {Design} of a {Next} {Generation} {MPI} {Implementation}},
	isbn = {9783540302186},
	shorttitle = {Open {MPI}},
	doi = {10.1007/978-3-540-30218-6_19},
	abstract = {A large number of MPI implementations are currently available, each of which emphasize different aspects of high-performance computing or are intended to solve a specific research problem. The result is a myriad of incompatible MPI implementations, all of which require separate installation, and the combination of which present significant logistical challenges for end users. Building upon prior research, and influenced by experience gained from the code bases of the LAM/MPI, LA-MPI, and FT-MPI projects, Open MPI is an all-new, production-quality MPI-2 implementation that is fundamentally centered around component concepts. Open MPI provides a unique combination of novel features previously unavailable in an open-source, production-quality implementation of MPI. Its component architecture provides both a stable platform for third-party research as well as enabling the run-time composition of independent software add-ons. This paper presents a high-level overview the goals, design, and implementation of Open MPI.},
	language = {en},
	booktitle = {Recent {Advances} in {Parallel} {Virtual} {Machine} and {Message} {Passing} {Interface}},
	publisher = {Springer},
	author = {Gabriel, Edgar and Fagg, Graham E. and Bosilca, George and Angskun, Thara and Dongarra, Jack J. and Squyres, Jeffrey M. and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and Castain, Ralph H. and Daniel, David J. and Graham, Richard L. and Woodall, Timothy S.},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	year = {2004},
	keywords = {Message Passing Interface, Component Architecture, Collective Operation, Component Framework, Reference Count},
	pages = {97--104},
}

@inproceedings{strazdins_application_2016,
	title = {Application {Fault} {Tolerance} for {Shrinking} {Resources} via the {Sparse} {Grid} {Combination} {Technique}},
	doi = {10.1109/IPDPSW.2016.210},
	abstract = {The need to make large-scale scientific simulations resilient to the shrinking and growing of compute resources arises from exascale computing and adverse operating conditions (fault tolerance). It can also arise from the cloudcomputing context where the cost of these resources can fluctuate. In this paper, we describe how the Sparse Grid Combination Technique can make such applications resilient to shrinking compute resources. The solution of the non-trivial issues of dealing with data redistribution and on-the-fly malleability of process grid information and ULFM MPI communicatorsare described. Results on a 2D advection solver indicate that process recovery time is significantly reduced from the alternate strategy where failed resources are replaced, overall execution time is actually improved from this case and for checkpointing and the execution error remains small, even when multiple failures occur.},
	booktitle = {2016 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Strazdins, Peter E. and Ali, Md. Mohsin and Debusschere, Bert},
	month = may,
	year = {2016},
	keywords = {Fault tolerance, Fault tolerant systems, Two dimensional displays, Handheld computers, Data structures, Computational modeling, Cloud computing, algorithm-based fault tolerance, ULFM, process failure recovery, PDE solvers, sparse grid combination technique, parallel computing, elasticity, cloud computing},
	pages = {1232--1238},
}

@inproceedings{engwer_high-level_2018,
	title = {A {High}-{Level} {C}++ {Approach} to {Manage} {Local} {Errors}, {Asynchrony} and {Faults} in an {MPI} {Application}},
	doi = {10.1109/PDP2018.2018.00117},
	abstract = {C++ advocates exceptions as the preferred way to handle unexpected behaviour of an implementation in the code. This does not integrate well with the error handling of MPI, which more or less always results in program termination in case of MPI failures. In particular, a local C++ exception can currently lead to a deadlock due to unfinished communication requests on remote hosts. At the same time, future MPI implementations are expected to include an API to continue computations even after a hard fault (node loss), i.e. the worst possible unexpected behaviour. In this paper we present an approach that adds extended exception propagation support to C++ MPI programs. Our technique allows to propagate local exceptions to remote hosts to avoid deadlocks, and to map MPI failures on remote hosts to local exceptions. A use case of particular interest are asynchronous 'local failure local recovery' resilience approaches. Our prototype implementation uses MPI-3.0 features only. In addition we present a dedicated implementation, which integrates seamlessly with MPI-ULFM, i.e. the most prominent proposal for extending MPI towards fault tolerance. Our implementation is available at https://gitlab.dune-project.org/christi/test-mpi-exceptions.},
	booktitle = {2018 26th {Euromicro} {International} {Conference} on {Parallel}, {Distributed} and {Network}-based {Processing} ({PDP})},
	author = {Engwer, Christian and Altenbernd, Mirco and Dreier, Nils-Arne and Göddeke, Dominik},
	month = mar,
	year = {2018},
	note = {ISSN: 2377-5750},
	keywords = {C++ languages, Standards, System recovery, Fault tolerance, Fault tolerant systems, Prototypes, Software, C++, ULFM, Exceptions, Fault-tolerance},
	pages = {714--721},
}

@inproceedings{gamell_evaluating_2016,
	title = {Evaluating {Online} {Global} {Recovery} with {Fenix} {Using} {Application}-{Aware} {In}-{Memory} {Checkpointing} {Techniques}},
	doi = {10.1109/ICPPW.2016.56},
	abstract = {Exascale systems promise the potential for computation atunprecedented scales and resolutions, but achieving exascale by theend of this decade presents significant challenges. A key challenge isdue to the very large number of cores and components and the resultingmean time between failures (MTBF) in the order of hours orminutes. Since the typical run times of target scientific applicationsare longer than this MTBF, fault tolerance techniques will beessential. An important class of failures that must be addressed isprocess or node failures. While checkpoint/restart (C/R) is currentlythe most widely accepted technique for addressing processor failures, coordinated, stable-storage-based global C/R might be unfeasible atexascale when the time to checkpoint exceeds the expected MTBF. This paper explores transparent recovery via implicitly coordinated, diskless, application-driven checkpointing as a way to tolerateprocess failures in MPI applications at exascale. The discussedapproach leverages User Level Failure Mitigation (ULFM), which isbeing proposed as an MPI extension to allow applications to createpolicies for tolerating process failures. Specifically, this paper demonstrates how different implementations ofapplication-driven in-memory checkpoint storage and recovery comparein terms of performance and scalability. We also experimentally evaluate the effectiveness and scalability ofthe Fenix online global recovery framework on a production system – the Titan Cray XK7 at ORNL – and demonstrate the ability of Fenix totolerate dynamically injected failures using the execution of fourbenchmarks and mini-applications with different behaviors.},
	booktitle = {2016 45th {International} {Conference} on {Parallel} {Processing} {Workshops} ({ICPPW})},
	author = {Gamell, Marc and Katz, Daniel S. and Teranishi, Keita and Heroux, Michael A. and Van der Wijngaart, Rob F. and Mattson, Timothy G. and Parashar, Manish},
	month = aug,
	year = {2016},
	note = {ISSN: 2332-5690},
	keywords = {Checkpointing, Fault tolerant systems, Redundancy, Runtime, Resilience, fault tolerance, resilience, online recovery, in-memory checkpointing, neighbor-based checkpointing, checksum-based checkpointing},
	pages = {346--355},
}

@inproceedings{schafer_extending_2020,
	title = {Extending the {MPI} {Stages} {Model} of {Fault} {Tolerance}},
	doi = {10.1109/ExaMPI52011.2020.00011},
	abstract = {Here, we expand upon our effective, checkpoint-based approach of fault tolerance for MPI-"MPI Stages," an extension of the Reinit model of fault-tolerant MPI introduced by some of us and others, notably without the use of setjmp/longjmp. MPI Stages saves internal MPI state in a separate checkpoint in coordination with application state saved by the application. While it is currently implemented based on the ExaMPI research implementation of MPI (designed to simplify checkpointing of state through a new, OO design), MPI Stages' downward requirement on any MPI implementation primarily inheres in internal-state checkpointability, apart from the new syntax and semantics of the MPI Stages model itself.As of now, MPI Stages supports communicators, groups, and limited forms of derived datatypes used with point-to-point and collective communication. We report on success with a substantial MPI application, SW4, which utilizes many of the common subset of features of many data-parallel MPI applications. We reinforce the model of a pre-main-type resilience interposition model, and introduce MPI opaque object serialization and deserialization. We also introduce new functions to better support use of the Stages model in hierarchical code and legacy software that does not sample MPI error codes faithfully. These MPI-Stages concepts appear useful for other fault tolerant MPI models and so are near-term standardization targets.We describe future steps needed to make MPI Stages more production ready, standardizable, and integrable with other MPI fault-tolerance models.},
	booktitle = {2020 {Workshop} on {Exascale} {MPI} ({ExaMPI})},
	author = {Schafer, Derek and Laguna, Ignacio and Skjellum, Anthony and Sultana, Nawrin and Mohror, Kathryn},
	month = nov,
	year = {2020},
	keywords = {Fault tolerant systems, Fault tolerance, Computational modeling, Standards, Runtime environment, Resilience, Load modeling, Fault Tolerance, Checkpoint-Restart, MPI},
	pages = {52--61},
}

@inproceedings{das_doomsday:_2018,
	address = {Dallas, Texas},
	series = {{SC} '18},
	title = {Doomsday: predicting which node will fail when on supercomputers},
	shorttitle = {Doomsday},
	url = {https://doi.org/10.1109/SC.2018.00012},
	doi = {10.1109/SC.2018.00012},
	abstract = {Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented. Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83\% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.},
	urldate = {2022-09-04},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage}, and {Analysis}},
	publisher = {IEEE Press},
	author = {Das, Anwesha and Mueller, Frank and Hargrove, Paul and Roman, Eric and Baden, Scott},
	month = nov,
	year = {2018},
	keywords = {failure analysis, machine learning, HPC},
	pages = {1--14},
}

@inproceedings{subasi_automatic_2017,
	address = {New York, NY, USA},
	series = {{ESPM2}'17},
	title = {Automatic {Risk}-based {Selective} {Redundancy} for {Fault}-tolerant {Task}-parallel {HPC} {Applications}},
	isbn = {9781450351331},
	url = {https://doi.org/10.1145/3152041.3152083},
	doi = {10.1145/3152041.3152083},
	abstract = {Silent data corruption (SDC) and fail-stop errors are the most hazardous error types in high-performance computing (HPC) systems. In this study, we present an automatic, efficient and lightweight redundancy mechanism to mitigate both error types. We propose partial task-replication and checkpointing for task-parallel HPC applications to mitigate silent and fail-stop errors. To avoid the prohibitive costs of complete replication, we introduce a lightweight selective replication mechanism. Using a fully automatic and transparent heuristics, we identify and selectively replicate only the reliability-critical tasks based on a risk metric. Our approach detects and corrects around 70\% of silent errors with only 5\% average performance overhead. Additionally, the performance overhead of the heuristic itself is negligible.},
	urldate = {2022-09-04},
	booktitle = {Proceedings of the {Third} {International} {Workshop} on {Extreme} {Scale} {Programming} {Models} and {Middleware}},
	publisher = {Association for Computing Machinery},
	author = {Subasi, Omer and Unsal, Osman and Krishnamoorthy, Sriram},
	month = nov,
	year = {2017},
	keywords = {task-parallelism, Fault-tolerance, dataflow, selective redundancy},
	pages = {1--8},
}

@inproceedings{hussain_partial_2018,
	address = {Dallas, Texas},
	series = {{SC} '18},
	title = {Partial redundancy in {HPC} systems with non-uniform node reliabilities},
	abstract = {We study the usefulness of partial redundancy in HPC message passing systems where individual node failure distributions are not identical. Prior research works on fault tolerance have generally assumed identical failure distributions for the nodes of the system. In such settings, partial replication has never been shown to outperform the two extremes(full and no-replication) for any significant range of node counts. In this work, we argue that partial redundancy may provide the best performance under the more realistic assumption of non-identical node failure distributions. We provide theoretical results on arranging nodes with different reliability values among replicas such that system reliability is maximized. Moreover, using system reliability to compute MTTI (mean-time-to-interrupt) and expected completion time of a partially replicated system, we numerically determine the optimal partial replication degree. Our results indicate that partial replication can be a more efficient alternative to full replication at system scales where Checkpoint/Restart alone is not sufficient.},
	urldate = {2022-09-04},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage}, and {Analysis}},
	publisher = {IEEE Press},
	author = {Hussain, Zaeem and Znati, Taieb and Melhem, Rami},
	month = nov,
	year = {2018},
	keywords = {fault tolerance, HPC, replication, checkpoint, resilience},
	pages = {1--11},
}

@inproceedings{wu_modeling_2018,
	address = {New York, NY, USA},
	series = {{ICPP} 2018},
	title = {Modeling {Application} {Resilience} in {Large}-scale {Parallel} {Execution}},
	isbn = {9781450365109},
	url = {https://doi.org/10.1145/3225058.3225119},
	doi = {10.1145/3225058.3225119},
	abstract = {Understanding how the application is resilient to hardware and software errors is critical to high-performance computing. To evaluate application resilience, the application level fault injection is the most common method. However, the application level fault injection can be very expensive when running the application in parallel in large scales due to the high requirement for hardware resource during fault injection. In this paper, we introduce a new methodology to evaluate the resilience of the application running in large scales. Instead of injecting errors into the application in large-scale execution, we inject errors into the application in small-scale execution and serial execution to model and predict the fault injection result for the application running in large scales. Our models are based on a series of empirical observations. Those observations characterize error occurrences and propagation across MPI processes in small-scale execution (including serial execution) and large-scale one. Our models achieve high prediction accuracy. Evaluating with four NAS parallel benchmarks and two proxy scientific applications, we demonstrate that using the fault injection result to predict for 64 MPI processes, the average prediction error is 8\%. While using the fault injection result to make the same prediction for eight MPI processes, the average prediction error decreases to 7\%.},
	urldate = {2022-09-09},
	booktitle = {Proceedings of the 47th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Kai and Dong, Wenqian and Guan, Qiang and DeBardeleben, Nathan and Li, Dong},
	month = aug,
	year = {2018},
	pages = {1--10},
}

@article{decyk_skeleton_1995,
	title = {Skeleton {PIC} codes for parallel computers},
	volume = {87},
	issn = {00104655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0010465594001693},
	doi = {10.1016/0010-4655(94)00169-3},
	language = {en},
	number = {1-2},
	urldate = {2023-03-29},
	journal = {Computer Physics Communications},
	author = {Decyk, Viktor K.},
	month = may,
	year = {1995},
	pages = {87--94},
}

@article{cappello_toward_2009,
	title = {Toward {Exascale} {Resilience}},
	volume = {23},
	issn = {1094-3420, 1741-2846},
	url = {http://journals.sagepub.com/doi/10.1177/1094342009347767},
	doi = {10.1177/1094342009347767},
	abstract = {Over the past few years resilience has became a major issue for high-performance computing (HPC) systems, in particular in the perspective of large petascale systems and future exascale systems. These systems will typically gather from half a million to several millions of central processing unit (CPU) cores running up to a billion threads. From the current knowledge and observations of existing large systems, it is anticipated that exascale systems will experience various kind of faults many times per day. It is also anticipated that the current approach for resilience, which relies on automatic or application level checkpoint/ restart, will not work because the time for checkpointing and restarting will exceed the mean time to failure of a full system. This set of projections leaves the community of fault tolerance for HPC systems with a difficult challenge: finding new approaches, which are possibly radically disruptive, to run applications until their normal termination, despite the essentially unstable nature of exascale systems. Yet, the community has only five to six years to solve the problem. This white paper synthesizes the motivations, observations and research issues considered as determinant of several complimentary experts of HPC in applications, programming models, distributed systems and system management.},
	language = {en},
	number = {4},
	urldate = {2023-03-30},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Cappello, Franck and Geist, Al and Gropp, Bill and Kale, Laxmikant and Kramer, Bill and Snir, Marc},
	month = nov,
	year = {2009},
	pages = {374--388},
}

@misc{noauthor_dup2_nodate,
	title = {dup(2) - {Linux} manual page},
	url = {https://man7.org/linux/man-pages/man2/dup.2.html},
	urldate = {2023-03-30},
}

@misc{noauthor_unix7_nodate,
	title = {unix(7) - {Linux} manual page},
	url = {https://man7.org/linux/man-pages/man7/unix.7.html},
	urldate = {2023-03-30},
}

@article{bailey_nas_1991,
	title = {The {Nas} {Parallel} {Benchmarks}},
	volume = {5},
	issn = {0890-2720},
	url = {http://journals.sagepub.com/doi/10.1177/109434209100500306},
	doi = {10.1177/109434209100500306},
	abstract = {A new set of benchmarks has been developed for the performance evaluation of highly parallel supercom puters. These consist of five "parallel kernel" bench marks and three "simulated application" benchmarks. Together they mimic the computation and data move ment characteristics of large-scale computational fluid dynamics applications. The principal distinguishing feature of these benchmarks is their "pencil and paper" specification—all details of these benchmarks are specified only algorithmically. In this way many of the difficulties associated with conventional bench- marking approaches on highly parallel systems are avoided.},
	language = {en},
	number = {3},
	urldate = {2023-03-30},
	journal = {The International Journal of Supercomputing Applications},
	author = {Bailey, D.H. and Barszcz, E. and Barton, J.T. and Browning, D.S. and Carter, R.L. and Dagum, L. and Fatoohi, R.A. and Frederickson, P.O. and Lasinski, T.A. and Schreiber, R.S. and Simon, H.D. and Venkatakrishnan, V. and Weeratunga, S.K.},
	month = sep,
	year = {1991},
	pages = {63--73},
}

@inproceedings{mallinson_cloverleaf:_2013,
	title = {{CloverLeaf}: {Preparing} {Hydrodynamics} {Codes} for {Exascale}},
	author = {Mallinson, A. C. and Beckingsale, D. A. and Gaudin, Wayne P. and Herdman, J. A. and Levesque, John M. and Jarvis, Stephen A.},
	year = {2013},
}

@article{panda_mvapich_2021,
	title = {The {MVAPICH} project: {Transforming} research into high-performance {MPI} library for {HPC} community},
	volume = {52},
	issn = {18777503},
	shorttitle = {The {MVAPICH} project},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877750320305093},
	doi = {10.1016/j.jocs.2020.101208},
	language = {en},
	urldate = {2023-03-30},
	journal = {Journal of Computational Science},
	author = {Panda, Dhabaleswar Kumar and Subramoni, Hari and Chu, Ching-Hsiang and Bayatpour, Mohammadreza},
	month = may,
	year = {2021},
	pages = {101208},
}

@article{bosilca_algorithm-based_2009,
	title = {Algorithm-based fault tolerance applied to high performance computing},
	volume = {69},
	issn = {07437315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731508002141},
	doi = {10.1016/j.jpdc.2008.12.002},
	language = {en},
	number = {4},
	urldate = {2023-03-31},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Bosilca, George and Delmas, Rémi and Dongarra, Jack and Langou, Julien},
	month = apr,
	year = {2009},
	pages = {410--416},
}

@inproceedings{fiala_detection_2012,
	address = {Salt Lake City, UT},
	title = {Detection and correction of silent data corruption for large-scale high-performance computing},
	isbn = {9781467308052 9781467308069},
	url = {http://ieeexplore.ieee.org/document/6468485/},
	doi = {10.1109/SC.2012.49},
	urldate = {2023-03-31},
	booktitle = {2012 {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Fiala, David and Mueller, Frank and Engelmann, Christian and Riesen, Rolf and Ferreira, Kurt and Brightwell, Ron},
	month = nov,
	year = {2012},
	pages = {1--12},
}

@inproceedings{moody_design_2010,
	title = {Design, {Modeling}, and {Evaluation} of a {Scalable} {Multi}-level {Checkpointing} {System}},
	doi = {10.1109/SC.2010.18},
	booktitle = {{SC} '10: {Proceedings} of the 2010 {ACM}/{IEEE} {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Moody, Adam and Bronevetsky, Greg and Mohror, Kathryn and Supinski, Bronis R. de},
	year = {2010},
	pages = {1--11},
}

@inproceedings{guermouche_uncoordinated_2011,
	title = {Uncoordinated {Checkpointing} {Without} {Domino} {Effect} for {Send}-{Deterministic} {MPI} {Applications}},
	doi = {10.1109/IPDPS.2011.95},
	booktitle = {Proceedings - 25th {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium}, {IPDPS} 2011},
	author = {Guermouche, Amina and Ropars, Thomas and Brunet, Elisabeth and Snir, Marc and Cappello, Franck},
	month = may,
	year = {2011},
	pages = {989--1000},
}

@inproceedings{guermouche_hydee:_2012,
	title = {{HydEE}: {Failure} {Containment} without {Event} {Logging} for {Large} {Scale} {Send}-{Deterministic} {MPI} {Applications}},
	doi = {10.1109/IPDPS.2012.111},
	booktitle = {2012 {IEEE} 26th {International} {Parallel} and {Distributed} {Processing} {Symposium}},
	author = {Guermouche, Amina and Ropars, Thomas and Snir, Marc and Cappello, Franck},
	year = {2012},
	pages = {1216--1227},
}

@inproceedings{riesen_alleviating_2012,
	address = {Salt Lake City, UT},
	title = {Alleviating scalability issues of checkpointing protocols},
	isbn = {9781467308052 9781467308069},
	url = {http://ieeexplore.ieee.org/document/6468460/},
	doi = {10.1109/SC.2012.18},
	urldate = {2023-03-31},
	booktitle = {2012 {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Riesen, Rolf and Ferreira, Kurt and Da Silva, Duma and Lemarinier, Pierre and Arnold, Dorian and Bridges, Patrick G.},
	month = nov,
	year = {2012},
	pages = {1--11},
}

@inproceedings{bouguerra_improving_2013,
	address = {Cambridge, MA, USA},
	title = {Improving the {Computing} {Efficiency} of {HPC} {Systems} {Using} a {Combination} of {Proactive} and {Preventive} {Checkpointing}},
	isbn = {9781467360661},
	url = {https://ieeexplore.ieee.org/document/6569837/},
	doi = {10.1109/IPDPS.2013.74},
	urldate = {2023-03-31},
	booktitle = {2013 {IEEE} 27th {International} {Symposium} on {Parallel} and {Distributed} {Processing}},
	publisher = {IEEE},
	author = {Bouguerra, Mohamed Slim and Gainaru, Ana and Gomez, Leonardo Bautista and Cappello, Franck and Matsuoka, Satoshi and Maruyama, Naoya},
	month = may,
	year = {2013},
	pages = {501--512},
}

@article{bougeret_using_2014,
	title = {Using group replication for resilience on exascale systems},
	volume = {28},
	issn = {1094-3420, 1741-2846},
	url = {http://journals.sagepub.com/doi/10.1177/1094342013505348},
	doi = {10.1177/1094342013505348},
	abstract = {High performance computing applications must be resilient to faults. The traditional fault-tolerance solution is checkpoint-recovery, by which application state is saved to and recovered from secondary storage throughout execution. It has been shown that, even when using an optimal checkpointing strategy, the checkpointing overhead precludes high parallel efficiency at large scale. Additional fault-tolerance mechanisms must thus be used. Such a mechanism is replication, that is, multiple processors performing the same computation so that a processor failure does not necessarily imply an application failure. In spite of resource waste, replication can lead to higher parallel efficiency when compared to using only checkpoint-recovery at large scale.
            We propose to execute and checkpoint multiple application instances concurrently, an approach we term group replication. For exponential failures we give an upper bound on the expected application execution time. This bound corresponds to a particular checkpointing period that we derive. For general failures, we propose a dynamic programming algorithm to determine non-periodic checkpoint dates as well as an empirical periodic checkpointing solution whose period is found via a numerical search. Using simulation we evaluate our proposed approaches, including comparison to the non-replication case, for both exponential and Weibull failure distributions. Our broad finding is that group replication is useful in a range of realistic application and checkpointing overhead scenarios for future exascale platforms.},
	language = {en},
	number = {2},
	urldate = {2023-03-31},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Bougeret, Marin and Casanova, Henri and Robert, Yves and Vivien, Frédéric and Zaidouni, Dounia},
	month = may,
	year = {2014},
	pages = {210--224},
}

@inproceedings{stearley_does_2012,
	title = {Does partial replication pay off?},
	doi = {10.1109/DSNW.2012.6264669},
	booktitle = {{IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN} 2012)},
	author = {Stearley, Jon and Ferreira, Kurt and Robinson, David and Laros, Jim and Pedretti, Kevin and Arnold, Dorian and Bridges, Patrick and Riesen, Rolf},
	year = {2012},
	pages = {1--6},
}

@inproceedings{lefray_replication_2013,
	address = {New York New York USA},
	title = {Replication for send-deterministic {MPI} {HPC} applications},
	isbn = {9781450319836},
	url = {https://dl.acm.org/doi/10.1145/2465813.2465819},
	doi = {10.1145/2465813.2465819},
	language = {en},
	urldate = {2023-03-31},
	booktitle = {Proceedings of the 3rd {Workshop} on {Fault}-tolerance for {HPC} at extreme scale},
	publisher = {ACM},
	author = {Lefray, Arnaud and Ropars, Thomas and Schiper, André},
	month = jun,
	year = {2013},
	pages = {33--40},
}

@inproceedings{ropars_efficient_2015,
	address = {Hyderabad, India},
	title = {Efficient {Process} {Replication} for {MPI} {Applications}: {Sharing} {Work} between {Replicas}},
	isbn = {9781479986491},
	shorttitle = {Efficient {Process} {Replication} for {MPI} {Applications}},
	url = {http://ieeexplore.ieee.org/document/7161552/},
	doi = {10.1109/IPDPS.2015.29},
	urldate = {2023-03-31},
	booktitle = {2015 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium}},
	publisher = {IEEE},
	author = {Ropars, Thomas and Lefray, Arnaud and Kim, Dohyun and Schiper, Andre},
	month = may,
	year = {2015},
	pages = {645--654},
}

@incollection{samfass_teampireplication-based_2020,
	title = {{TeaMPI}—{Replication}-{Based} {Resilience} {Without} the ({Performance}) {Pain}},
	isbn = {978-3-030-50742-8},
	author = {Samfass, Philipp and Weinzierl, Tobias and Hazelwood, Benjamin and Bader, Michael},
	month = jun,
	year = {2020},
	doi = {10.1007/978-3-030-50743-5_23},
	pages = {455--473},
}
